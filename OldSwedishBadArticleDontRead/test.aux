\relax 
\catcode `"\active 
\select@language{swedish}
\@writefile{toc}{\select@language{swedish}}
\@writefile{lof}{\select@language{swedish}}
\@writefile{lot}{\select@language{swedish}}
\citation{cs231n}
\@writefile{toc}{\contentsline {section}{\numberline {1}Inledning}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Bakgrund}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Syfte}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Fr\IeC {\r a}gest\IeC {\"a}llning}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Metod}{5}}
\citation{cs231n}
\citation{cs231n}
\citation{cs231n}
\citation{cs231n}
\citation{wikiStanford}
\citation{cs231n}
\citation{wikiStanford}
\@writefile{toc}{\contentsline {section}{\numberline {3}Resultat}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Tensorer, indexering och notation}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Feed-forward neurala n\IeC {\"a}tverk}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Fram\IeC {\r a}tprogagering}{6}}
\citation{cs231n}
\citation{wikiStanford}
\citation{cs231n}
\citation{wikiStanford}
\citation{cs231n}
\newlabel{figFCC}{{3.2}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Ett exempel p\IeC {\r a} ett enkelt feed-forward neuralt n\IeC {\"a}tverk. Inputneuronerna \IeC {\"a}r bl\IeC {\r a}markerade medan resterande neuroner \IeC {\"a}r orangea. R\IeC {\"o}da neuroner \IeC {\"a}r s\IeC {\r a} kallade \textit  {bias-neuroner} som \IeC {\"a}r konstanta oberoende p\IeC {\r a} inmatningsdatan. Svarta linjer symboliserar vikterna och styrkan mellan tv\IeC {\r a} neuroner.}}{7}}
\newlabel{feed-forward}{{3}{7}}
\citation{cs231n}
\citation{wikiStanford}
\newlabel{aktiveringsfunktion}{{3.2.1}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Grafen av aktiveringsfunktionerna $ReLU$, $\sigma $ och $tanh$.}}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Kostnadsfunktionen}{8}}
\citation{gradient}
\citation{convmath}
\citation{gradient}
\citation{convmath}
\citation{wikiStanford}
\citation{figSGD}
\citation{gradient}
\citation{convmath}
\citation{wikiStanford}
\citation{wikiStanford}
\citation{gradient}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Gradient Descent}{9}}
\newlabel{SGD}{{12}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Bak\IeC {\r a}tpropagering}{9}}
\citation{cs231n}
\citation{cs231n}
\newlabel{figSGD}{{3.2.3}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces En illustration av gradient descent p\IeC {\r a} en funktion med tv\IeC {\r a} variabler.\cite  {figSGD}}}{10}}
\newlabel{dLdW_FCC}{{15}{10}}
\citation{cs231n}
\citation{cs231n}
\citation{cs231n}
\citation{cs231n}
\citation{cs231n}
\newlabel{dLdb_FCC}{{16}{11}}
\newlabel{dLdX_FCC}{{18}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Tr\IeC {\"a}ning av neurala n\IeC {\"a}tverk}{11}}
\citation{cs231n}
\citation{cs231n}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Konvolutionella neuala n\IeC {\"a}tverk}{12}}
\newlabel{figkatter}{{3.4}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Resultatet av att ett filter f\IeC {\"o}r vertikal respektive horizontell kantdetektering har sammanrullat \IeC {\"o}ver en bild av en katt.}}{12}}
\citation{cs231n}
\citation{convmath}
\citation{cs231n}
\citation{cs231n}
\citation{convmath}
\citation{cs231n}
\citation{convmath}
\citation{figboatcnn}
\citation{figboatcnn}
\citation{cs231n}
\citation{convmath}
\citation{cs231n}
\citation{convmath}
\newlabel{CNNeq}{{19}{13}}
\citation{cs231n}
\citation{convmath}
\citation{figkonv}
\citation{figkonv}
\citation{cs231n}
\citation{convmath}
\citation{cs231n}
\citation{convmath}
\citation{convarithmetic}
\newlabel{figboatcnn}{{3.4}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces En illustration av ett konvolutionellt neuralt n\IeC {\"a}tverk. Varje skiva \IeC {\"a}r en egen \textit  {feature map}. \cite  {figboatcnn}}}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Konvolutionslagret fram\IeC {\r a}tpropagering}{14}}
\newlabel{figkonv}{{3.4.1}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces En k\IeC {\"a}rna med storlek $3 \times 3$ sammanrullar \IeC {\"o}ver ett omr\IeC {\r a}de med dimensioner $6 \times 6$ och bildar en aktivering med dimensionerna $4 \times 4$. \cite  {figkonv}}}{14}}
\citation{cs231n}
\citation{convmath}
\citation{convarithmetic}
\citation{cs231n}
\citation{convmath}
\citation{cs231n}
\citation{convmath}
\citation{convarithmetic}
\newlabel{figzeropad}{{3.4.1}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Ett omr\IeC {\r a}de med dimensioner $3 \times 3$ zero-paddas med $p=1$ och resulterande omr\IeC {\r a}de f\IeC {\r a}r dimensioner $5 \times 5$.}}{15}}
\newlabel{konvolution}{{24}{15}}
\citation{convmath}
\citation{webconv1}
\citation{webconv2}
\citation{webconv3}
\citation{webconv1}
\citation{webconv2}
\citation{webconv3}
\citation{webconv1}
\citation{webconv2}
\citation{webconv3}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Konvolutionslagret bak\IeC {\r a}tpropagering}{16}}
\newlabel{konvolutionbackprop}{{25}{16}}
\citation{cs231n}
\citation{cs231n}
\citation{webconv1}
\citation{webconv2}
\citation{webconv3}
\citation{convmath}
\citation{cs231n}
\citation{cs231n}
\citation{convmath}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Aktiveringsfunktionslager fram\IeC {\r a}tpropagering}{17}}
\citation{cs231n}
\citation{convmath}
\citation{convarithmetic}
\citation{cs231n}
\citation{convmath}
\citation{convarithmetic}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.4}Aktiveringsfunktionslager bak\IeC {\r a}tpropagering}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.5}Maxpoollagret fram\IeC {\r a}tpropagation}{18}}
\newlabel{figmaxpool}{{3.4.5}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Maxpooling med $k=2$ och $s=2$ av ett omr\IeC {\r a}de med dimensioner $4 \times 4$ d\IeC {\"a}r resultatet bildar ett omr\IeC {\r a}de med dimensionerna $2 \times 2$.}}{18}}
\citation{cs231n}
\citation{convmath}
\citation{cs231n}
\citation{convmath}
\citation{cs231n}
\citation{convmath}
\citation{webconv3}
\citation{cs231n}
\citation{convmath}
\citation{webconv3}
\citation{cs231n}
\citation{convmath}
\citation{webconv3}
\citation{cs231n}
\citation{batchnorm}
\newlabel{maxpool}{{34}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.6}Maxpoollagret bak\IeC {\r a}tpropagering}{19}}
\citation{cs231n}
\citation{batchnorm}
\citation{cs231n}
\citation{batchnorm}
\citation{cs231n}
\citation{batchnorm}
\citation{cs231n}
\citation{batchnorm}
\citation{cs231n}
\citation{batchnorm}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.7}Batch Normalization fram\IeC {\r a}tpropagering}{20}}
\citation{webBN1}
\citation{webBN2}
\citation{webBN1}
\citation{webBN2}
\citation{webBN1}
\citation{webBN2}
\citation{webBN1}
\citation{webBN2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.8}Batch Normalization bak\IeC {\r a}tpropagering}{21}}
\newlabel{kroneckerdelta}{{43}{21}}
\newlabel{kroneckerdeltaDERIVATIVE}{{44}{21}}
\newlabel{kroneckerdeltaSUM}{{45}{21}}
\newlabel{BN_delta_error}{{46}{21}}
\newlabel{BN_dxdxhat}{{47}{21}}
\citation{webBN1}
\citation{webBN2}
\citation{webBN1}
\citation{webBN2}
\newlabel{BN_kedjeregeln}{{48}{22}}
\newlabel{mu'}{{49}{22}}
\newlabel{sigma'}{{50}{22}}
\citation{webBN1}
\citation{webBN2}
\newlabel{finalBNeq}{{51}{23}}
\citation{cs231n}
\citation{cs231n}
\citation{notesonbackprop}
\citation{websoftmax}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.9}Softmax fram\IeC {\r a}tpropagering}{24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.10}Softmax bak\IeC {\r a}tpropagering}{24}}
\citation{MNIST}
\citation{cs231n}
\citation{MNIST}
\citation{cs231n}
\citation{notesonbackprop}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Praktiska Till\IeC {\"a}mpningar}{25}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Klassificering av handskrivna siffror}{25}}
\citation{cs231n}
\citation{retinanet}
\newlabel{figMNIST}{{3.5.1}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Tio bilder av handskrivna siffror fr\IeC {\r a}n MNISTdatabasen. \cite  {MNIST}}}{26}}
\newlabel{MNISTmodel}{{3.5.1}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Modellen som anv\IeC {\"a}ndes f\IeC {\"o}r att l\IeC {\"a}sa av handskrivna siffror. Batch Normalization ben\IeC {\"a}mns med BN, konvolution menas en konvolution med k\IeC {\"a}rnstorlek k = 5 och stride s = 2. ReLU \IeC {\"a}r aktiveringsfunktionen Rectified Linear Units och softmax \IeC {\"a}r softmaxlagret.}}{26}}
\citation{cs231n}
\citation{iou}
\citation{iou}
\citation{iou}
\citation{resnet}
\citation{retinanet}
\citation{fpn}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Ansiktsigenk\IeC {\"a}nning}{27}}
\newlabel{figiou}{{3.5.2}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces IoU definieras som storleken av snittet av tv\IeC {\r a} m\IeC {\"a}nger A och B dividerat med storleken av unionen av A och B. En st\IeC {\"o}rre IoU medf\IeC {\"o}r att prognosen \IeC {\"a}r n\IeC {\"a}rmare sanningen. \cite  {iou}}}{27}}
\citation{retinanet}
\citation{retinanet}
\citation{resnet}
\citation{retinanet}
\citation{fpn}
\citation{retinanet}
\newlabel{figiou}{{3.5.2}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces RetinaNets arkitektur fr\IeC {\r a}n \textit  {Focal Loss for Dense Object Detection} (Lin et al.). Varje pyramidniv\IeC {\r a} matas in i ett klassifikationshuvud och ett regressionshuvud. \cite  {retinanet}}}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Diskussion}{29}}
\newlabel{results1}{{3.5.2}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Kvantitativa resultat fr\IeC {\r a}n valideringsdatan fr\IeC {\r a}n WIDERFace: Bilder som modellen aldrig har sett tidigare.}}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {5}K\IeC {\"a}llkritik}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}CS231n: Convolutional Neural Networks for Visual Recognition}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Batch normalization: Accelerating deep network training by reducing internal covariate shift.}{31}}
\bibcite{cs231n}{1}
\bibcite{notesonbackprop}{2}
\bibcite{convmath}{3}
\bibcite{convarithmetic}{4}
\bibcite{highperformanceconv}{5}
\bibcite{wikiStanford}{6}
\bibcite{gradient}{7}
\bibcite{yolo}{8}
\bibcite{batchnorm}{9}
\bibcite{webconv1}{10}
\bibcite{webBN1}{11}
\bibcite{webconv2}{12}
\bibcite{webBN2}{13}
\bibcite{webconv3}{14}
\bibcite{websoftmax}{15}
\bibcite{MNIST}{16}
\bibcite{figSGD}{17}
\bibcite{figboatcnn}{18}
\bibcite{figkonv}{19}
\bibcite{figconv}{20}
\bibcite{resnet}{21}
\bibcite{iou}{22}
\bibcite{retinanet}{23}
\bibcite{fpn}{24}
\bibcite{WIDERFace}{25}
